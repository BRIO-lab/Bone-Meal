{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sasank/miniconda3/envs/jtml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class debug_Configuration:\n",
    "    def __init__(self):\n",
    "        # CWDE: self.init MUST BE LISTED FIRST\n",
    "        self.init = {\n",
    "            'PROJECT_NAME': 'Segmentation Trial',\n",
    "            'MODEL_NAME': 'MyModel',\n",
    "            'RUN_NAME': time.strftime('%Y-%m-%d-%H-%M-%S'),\n",
    "            'WANDB_RUN_GROUP': 'miller-lab',\n",
    "            'FAST_DEV_RUN': False,  # Runs inputted batches (True->1) and disables logging and some callbacks\n",
    "            'MAX_EPOCHS': 25,\n",
    "            'MAX_STEPS': -1,    # -1 means it will do all steps and be limited by epochs\n",
    "            'STRATEGY': None    # This is the training strategy. Should be 'ddp' for multi-GPU (like HPG)\n",
    "        }\n",
    "        self.etl = {\n",
    "            'RAW_DATA_FILE': -1,    # -1 means it will create a full data csv from the image directory, using all images in the image directory\n",
    "            #'RAW_DATA_FILE': 'my_data.csv',\n",
    "            'DATA_DIR': \"data\",\n",
    "            'VAL_SIZE':  0.1,       # looks sus\n",
    "            'TEST_SIZE': 0.1,      # I'm not sure these two mean what we think\n",
    "            #'random_state': np.random.randint(1,50)\n",
    "            # HHG2TG lol; deterministic to aid reproducibility\n",
    "            'RANDOM_STATE': 42,\n",
    "\n",
    "            'CUSTOM_TEST_SET': False,\n",
    "            'TEST_SET_NAME': '/my/test/set.csv'\n",
    "        }\n",
    "\n",
    "        self.dataset = {\n",
    "            'DATA_NAME': 'Ten_Dogs_64KP',\n",
    "            'IMAGE_HEIGHT': 1024,\n",
    "            'IMAGE_WIDTH': 1024,\n",
    "            'MODEL_TYPE': 'tib',        # specifies that it's a femur model. how should we do this? not clear this is still best...\n",
    "            'CLASS_LABELS': {0: 'bone', 1: 'background'},\n",
    "            'IMG_CHANNELS': 1,      # Is this differnt from self.module['NUM_IMAGE_CHANNELS']\n",
    "            'IMAGE_THRESHOLD': 0,\n",
    "            'SUBSET_PIXELS': True,\n",
    "            'USE_ALBUMENTATIONS': True,\n",
    "            'NUM_KEY_POINTS' : 64,\n",
    "        }\n",
    "\n",
    "        self.datamodule = {\n",
    "            # *** CHANGE THE IMAGE DIRECTORY TO YOUR OWN ***\n",
    "            'IMAGE_DIRECTORY': '/media/sasank/LinuxStorage/Dropbox (UFL)/Canine Kinematics Data/TPLO_Ten_Dogs_grids',\n",
    "            \n",
    "            # Z. Curran:  '/home/curran.z/blue_zhe.jiang/curran.z/BM/images'\n",
    "            # CWDE: \"C:/Users/cwell/Documents/jtml_data/TPLO_Ten_Dogs_grids\"\n",
    "            # CWDE: '/home/driggersellis.cw/jtml_data/TPLO_Ten_Dogs_grids/' \n",
    "            \n",
    "            #'IMAGE_DIRECTORY': 'C:/Users/cwell/Documents/jtml_data/TPLO_Ten_Dogs_grids',\n",
    "            # *** CHANGE THE CHECKPOINT PATH TO YOUR OWN FOR TESTING ***\n",
    "            #'CKPT_FILE': 'path/to/ckpt/file.ckpt',  # used when loading model from a checkpoint\n",
    "            # used when loading model from a checkpoint, such as in testing\n",
    "            \n",
    "            # Z. Curran : '/home/curran.z/blue_zhe.jiang/curran.z/BM/Bone-Meal/checkpoints/'\n",
    "            # CWDE: \"C:/Users/cwell/Documents/jtml_data/Checkpoints/\"\n",
    "            # CWDE: '/home/driggersellis.cw/jtml_data/Bone-Meal/checkpoints/' \n",
    "            \n",
    "            'CKPT_FILE': 'C:/Users/cwell/Documents/jtml_data/Checkpoints/' + self.init['WANDB_RUN_GROUP'] + self.init['MODEL_NAME'] + '.ckpt', \n",
    "            'BATCH_SIZE': 4,\n",
    "            'SHUFFLE': True,        # Only for training, for test and val this is set in the datamodule script to False\n",
    "            'NUM_WORKERS': 4,   # This number seems fine for local but on HPG, we have so many cores that a number like 4 seems better.\n",
    "            'PIN_MEMORY': False,\n",
    "            #'SUBSET_PIXELS': True,\n",
    "            'USE_NAIVE_TEST_SET': False\n",
    "        }\n",
    "\n",
    "        # hyperparameters for training\n",
    "        self.hparams = {\n",
    "            'LOAD_FROM_CHECKPOINT': False,\n",
    "            'learning_rate': 1e-3\n",
    "        }\n",
    "        \n",
    "        \n",
    "        # network params\n",
    "        self.net = {\n",
    "            # 'hrt_small', 'hrnet'\n",
    "            'BACKBONE': 'hrt_small', # the name of the backbone identified in backbone_selector. Currently have planned support for hrt and hrnet\n",
    "            # 'seg_hrt', 'seg_hrnet'\n",
    "            'ARCHITECTURE' :'seg_hrt', # name of the architecture_builder class file\n",
    "            'DATA_MODULE' : 'segmentation_data_module'\n",
    "        }\n",
    "        \n",
    "        # PARAMS FOR BACKBONES (Format: self.[name of backbone in self.net] = { params dict })\n",
    "        \n",
    "        # these are essentially params for the hrnet backbone's SegmentationNetModule class\n",
    "        # they are an exception to the format established in the comment above.\n",
    "        self.segmentation_net_module = {\n",
    "                'NUM_IMG_CHANNELS': self.dataset['IMG_CHANNELS'],\n",
    "                'LOSS' : 'torch_nn_bce_with_logits_loss'\n",
    "        }\n",
    "        \n",
    "        # Params for HRT's segmentation_net_module. Defaults used from HRT's Base config\n",
    "        self.hrt_segmentation_net = {\n",
    "                'MODEL_CONFIG' : 'hrt_small',\n",
    "                'LOSS' : 'torch_nn_bce_loss'\n",
    "        }\n",
    "        \n",
    "        # PARAMS FOR LOSS FUNCTIONS (Format: self.[name of loss in self.backbone] = { params dict })\n",
    "        \n",
    "        # Params dict for BCEWithLogitsLoss, which takes no params in the origin model from Lightning Segmentation.\n",
    "        self.torch_nn_bce_with_logits_loss = {\n",
    "            # NO PARAMS\n",
    "        }\n",
    "\n",
    "        self.torch_nn_bce_loss = {\n",
    "            # NO PARAMS\n",
    "        }\n",
    "        \n",
    "        self.ohem_ce_loss = {\n",
    "            'IGNORE_LABEL' : -1,\n",
    "            'THRES' : 0.7,\n",
    "            'MIN_KEPT' : 100000,\n",
    "            'WEIGHT' : None\n",
    "        }\n",
    "        \n",
    "        # Params for FSCELoss: TODO: insert actual params\n",
    "        self.fsce_loss = {\n",
    "            'ce_weight' : -1,\n",
    "            'ce_reduction' : -1,\n",
    "            'ce_ignore_index': -1\n",
    "        }\n",
    "        \n",
    "        #TODO: add other params dicts for each loss function we have. Code will be extensible \n",
    "\n",
    "        # Commented out transforms do not support keypoints\n",
    "        self.transform = \\\n",
    "        A.Compose([\n",
    "        A.RandomGamma(always_apply=False, p = 0.5,gamma_limit=(10,300)),\n",
    "        A.ShiftScaleRotate(always_apply = False, p = 0.5,shift_limit=(-0.06, 0.06), scale_limit=(-0.1, 0.1), rotate_limit=(-180,180), interpolation=0, border_mode=0, value=(0, 0, 0)),\n",
    "        A.Blur(always_apply=False, blur_limit=(3, 10), p=0.2),\n",
    "        A.Flip(always_apply=False, p=0.5),\n",
    "        # A.ElasticTransform(always_apply=False, p=0.85, alpha=0.5, sigma=150, alpha_affine=50.0, interpolation=0, border_mode=0, value=(0, 0, 0), mask_value=None, approximate=False),\n",
    "        A.InvertImg(always_apply=False, p=0.5),\n",
    "        A.CoarseDropout(always_apply = False, p = 0.25, min_holes = 1, max_holes = 100, min_height = 25, max_height=25),\n",
    "        A.MultiplicativeNoise(always_apply=False, p=0.25, multiplier=(0.1, 2), per_channel=True, elementwise=True)\n",
    "    ],\n",
    "    keypoint_params=A.KeypointParams(format='xy', remove_invisible=False),\n",
    "    p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class debug_LitJTMLDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, config, evaluation_type, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config (config): Dictionary of vital constants about data.\n",
    "            store_data_ram (boolean): Taken from config.\n",
    "            evaluation_type (string): Dataset evaluation type (must be 'training', 'validation', or 'test')\n",
    "            num_points (int): Taken from config.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        # Create local copies of the arguments\n",
    "        self.config = config\n",
    "        self.num_points = self.config.dataset['NUM_KEY_POINTS']\n",
    "        self.transform = self.config.transform\n",
    "        \n",
    "        # Check that evaluation_type is valid and then store\n",
    "        if evaluation_type in ['train', 'val', 'test', 'naive']:\n",
    "            self.evaluation_type = evaluation_type\n",
    "        else:\n",
    "            raise Exception('Incorrect evaluation type! Must be either \\'train\\', \\'val\\', \\'test\\', or \\'naive\\'.')\n",
    "\n",
    "        # Load the data from the big_data CSV file into a pandas dataframe\n",
    "        self.data = pd.read_csv(os.path.join(self.config.etl['DATA_DIR'], self.config.dataset['DATA_NAME'], self.evaluation_type + '_' + self.config.dataset['DATA_NAME'] + '.csv'))\n",
    "        \n",
    "    \n",
    "    \n",
    "    #def __init__(self, config, dataset, img_dir):\n",
    "\n",
    "        \"\"\"\n",
    "        # image check\n",
    "        #print('Image directory: ' + self.config.data_constants[\"IMAGE_DIRECTORY\"])\n",
    "        for idx in range(0,len(self.images)):\n",
    "            if os.path.isfile(self.img_dir + '/' + self.images[idx]) ==False:\n",
    "                raise Exception('Error, cannot find file: ' + self.images[idx])\n",
    "        \n",
    "        #print(self.config.data_constants['MODEL_TYPE'])\n",
    "        for i,j in enumerate(self.dataset[0,:]):\n",
    "            #if j == self.config.data_constants['MODEL_TYPE']:\n",
    "            if j == 'fem':\n",
    "                self.labels = self.dataset[1:,i]\n",
    "        \n",
    "        # label check\n",
    "        for idx in range(0,len(self.labels)):\n",
    "            if os.path.isfile(self.img_dir + '/' + self.labels[idx]) ==False:\n",
    "                raise Exception('Error, cannot find file: ' + self.labels[idx])\n",
    "        \"\"\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get the row of the dataframe\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Get the image name\n",
    "        image_name = row['Image address']\n",
    "\n",
    "        # Get the image\n",
    "        image = io.imread(os.path.join(self.config.datamodule['IMAGE_DIRECTORY'], image_name))\n",
    "\n",
    "        # Get the keypoint labels and segmentation labels\n",
    "        if self.config.dataset['MODEL_TYPE'] == 'fem':\n",
    "            kp_label = row['Femur 2D KP points']\n",
    "            seg_label = io.imread(os.path.join(self.config.datamodule['IMAGE_DIRECTORY'], row['Fem label address']))\n",
    "        elif self.config.dataset['MODEL_TYPE'] == 'tib':\n",
    "            kp_label = row['Tibia 2D KP points']\n",
    "            seg_label = io.imread(os.path.join(self.config.datamodule['IMAGE_DIRECTORY'], row['Tib label address']))\n",
    "        else:\n",
    "            raise Exception('Incorrect model type! Must be either \\'fem\\' or \\'tib\\'.')\n",
    "\n",
    "        kp_label = kp_label[2:-2]\n",
    "        kp_label = kp_label.split(']' + os.linesep + ' [')\n",
    "        kp_label = [np.array([float(x) for x in list(filter(None, kp.split(' ')))]) for kp in kp_label]\n",
    "        kp_label = np.array(kp_label)\n",
    "        kp_label[:, 1] = 1 - kp_label[:, 1]         # ! New kp_label preprocessing\n",
    "        kp_label = kp_label * 1024\n",
    "        \n",
    "\n",
    "        # * Transformations\n",
    "        # Albumenations\n",
    "        image_no_transform = image\n",
    "        if self.transform and self.config.dataset['USE_ALBUMENTATIONS'] == True:\n",
    "            transformed = self.transform(image=image, mask=seg_label, keypoints=kp_label)\n",
    "            image, seg_label, kp_label = transformed['image'], transformed['mask'], transformed['keypoints']\n",
    "\n",
    "        # * Subset Pixels\n",
    "        full_image = image             # Save full image (no subset_pixels) for visualization\n",
    "        if self.config.dataset['SUBSET_PIXELS'] == True:\n",
    "            label_dst = np.zeros_like(seg_label)\n",
    "            label_normed = cv2.normalize(seg_label, label_dst, alpha = 0, beta = 1, norm_type = cv2.NORM_MINMAX)\n",
    "            seg_label = label_normed\n",
    "\n",
    "            kernel = np.ones((30,30), np.uint8)\n",
    "            label_dilated = cv2.dilate(seg_label, kernel, iterations = 5)\n",
    "            image_subsetted = cv2.multiply(label_dilated, image)\n",
    "            image = image_subsetted\n",
    "\n",
    "        # * Convert to tensors\n",
    "        image = torch.FloatTensor(image[None, :, :]) # Store as byte (to save space) then convert when called in __getitem__. - What. What does this mean?\n",
    "        full_image = torch.FloatTensor(full_image[None, :, :]) # Store as byte (to save space) then convert when called in __getitem__\n",
    "        seg_label = torch.FloatTensor(seg_label[None, :, :])\n",
    "        #kp_label = torch.FloatTensor(kp_label.reshape(-1))      # Reshape to 1D array so that it's 2*num_keypoints long\n",
    "        kp_label = torch.FloatTensor(kp_label)          # kp_label is of shape (num_keypoints, 2)\n",
    "\n",
    "        # CWDE: Removed so that segmentation can occur without implementing the KP data fully\n",
    "        #assert (kp_label.shape[0], kp_label.shape[1]) == (self.num_points, 2), \"Keypoint label shape is incorrect!\"\n",
    "        #assert (kp_label.shape[0], kp_label.shape[1]) == (self.num_points, 2), \"index \" + str(idx) + \" kp_label.shape: \" + str(kp_label.shape) + \" self.num_points: \" + str(self.num_points)\n",
    "        assert len(kp_label) == self.num_points, \"index \" + str(idx) + \" kp_label.shape: \" + str(kp_label.shape) + \" self.num_points: \" + str(self.num_points)\n",
    "\n",
    "        #print(\"kp_label.shape:\")\n",
    "        #print(kp_label.shape)\n",
    "\n",
    "        # * Create a dictionary of the sample\n",
    "        sample = {'image': image,\n",
    "                    'img_name': image_name,\n",
    "                    'kp_label': kp_label,\n",
    "                    'seg_label': seg_label,\n",
    "                    'full_image': full_image,\n",
    "                    'image_no_transform': image_no_transform}\n",
    "        \n",
    "        # CWDE: hotfix for compatibility with segmentation code that just uses 'label'\n",
    "        sample['label'] = seg_label\n",
    "\n",
    "        # * Return the sample\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory to the root of the git repo since that's what the config file expects\n",
    "os.chdir('/home/sasank/Documents/GitRepos/Bone-Meal/')\n",
    "config = debug_Configuration()\n",
    "dataset = debug_LitJTMLDataset(config, evaluation_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "index 6 kp_label.shape: torch.Size([63, 2]) self.num_points: 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# For each dataset element, print the number of keypoints and the shape of the keypoints\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(dataset)):\n\u001b[1;32m      3\u001b[0m     \u001b[39m#print(dataset[i]['kp_label'].shape)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mif\u001b[39;00m dataset[i][\u001b[39m'\u001b[39m\u001b[39mkp_label\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (dataset\u001b[39m.\u001b[39mnum_points, \u001b[39m2\u001b[39m):\n\u001b[1;32m      5\u001b[0m         \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m      6\u001b[0m         \u001b[39mprint\u001b[39m(dataset[i][\u001b[39m'\u001b[39m\u001b[39mkp_label\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[41], line 110\u001b[0m, in \u001b[0;36mdebug_LitJTMLDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    105\u001b[0m kp_label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(kp_label)          \u001b[39m# kp_label is of shape (num_keypoints, 2)\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m# CWDE: Removed so that segmentation can occur without implementing the KP data fully\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m#assert (kp_label.shape[0], kp_label.shape[1]) == (self.num_points, 2), \"Keypoint label shape is incorrect!\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m#assert (kp_label.shape[0], kp_label.shape[1]) == (self.num_points, 2), \"index \" + str(idx) + \" kp_label.shape: \" + str(kp_label.shape) + \" self.num_points: \" + str(self.num_points)\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(kp_label) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_points, \u001b[39m\"\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(idx) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m kp_label.shape: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(kp_label\u001b[39m.\u001b[39mshape) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m self.num_points: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_points)\n\u001b[1;32m    112\u001b[0m \u001b[39m#print(\"kp_label.shape:\")\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m#print(kp_label.shape)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[39m# * Create a dictionary of the sample\u001b[39;00m\n\u001b[1;32m    116\u001b[0m sample \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m: image,\n\u001b[1;32m    117\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mimg_name\u001b[39m\u001b[39m'\u001b[39m: image_name,\n\u001b[1;32m    118\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mkp_label\u001b[39m\u001b[39m'\u001b[39m: kp_label,\n\u001b[1;32m    119\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mseg_label\u001b[39m\u001b[39m'\u001b[39m: seg_label,\n\u001b[1;32m    120\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mfull_image\u001b[39m\u001b[39m'\u001b[39m: full_image,\n\u001b[1;32m    121\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mimage_no_transform\u001b[39m\u001b[39m'\u001b[39m: image_no_transform}\n",
      "\u001b[0;31mAssertionError\u001b[0m: index 6 kp_label.shape: torch.Size([63, 2]) self.num_points: 64"
     ]
    }
   ],
   "source": [
    "# For each dataset element, print the number of keypoints and the shape of the keypoints\n",
    "for i in range(len(dataset)):\n",
    "    #print(dataset[i]['kp_label'].shape)\n",
    "    if dataset[i]['kp_label'].shape != (dataset.num_points, 2):\n",
    "        print(i)\n",
    "        print(dataset[i]['kp_label'].shape)\n",
    "        print((dataset.num_points, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset CSV\n",
    "dataset_csv = pd.read_csv(\"/home/sasank/Documents/GitRepos/Bone-Meal/data/Ten_Dogs_64KP/train_Ten_Dogs_64KP.csv\")\n",
    "\n",
    "# Print the number of keypoints and the shape of the keypoints for each row\n",
    "for i in range(len(dataset_csv)):\n",
    "    kp_label = dataset_csv.iloc[i]['Tibia 2D KP points']\n",
    "    kp_label = kp_label[2:-2]\n",
    "    kp_label = kp_label.split(']' + os.linesep + ' [')\n",
    "    kp_label = [np.array([float(x) for x in list(filter(None, kp.split(' ')))]) for kp in kp_label]\n",
    "    kp_label = np.array(kp_label)\n",
    "    kp_label[:, 1] = 1 - kp_label[:, 1]         # ! New kp_label preprocessing\n",
    "    kp_label = kp_label * 1024\n",
    "    if kp_label.shape != (64, 2):\n",
    "        print(i)\n",
    "        print(kp_label.shape)\n",
    "        print((64, 2))\n",
    "    #print(kp_label.shape)\n",
    "    #print((dataset.num_points, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
