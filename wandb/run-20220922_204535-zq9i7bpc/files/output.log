GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/sasank/miniconda3/envs/sasank-jtml-env/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /media/sasank/LinuxStorage/Dropbox (UFL)/LitJTML/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Epoch 0:   0%|                                                                                                           | 2/2058 [00:00<14:38,  2.34it/s, loss=0.676, v_num=18]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name       | Type                  | Params
-----------------------------------------------------
0 | pose_hrnet | PoseHighResolutionNet | 9.3 M
1 | loss_fn    | BCEWithLogitsLoss     | 0
-----------------------------------------------------
9.3 M     Trainable params
0         Non-trainable params
9.3 M     Total params
37.200    Total estimated model params size (MB)
/home/sasank/miniconda3/envs/sasank-jtml-env/lib/python3.9/site-packages/torch/nn/functional.py:3609: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/sasank/miniconda3/envs/sasank-jtml-env/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:381: UserWarning: `ModelCheckpoint(monitor='validation/loss')` could not find the monitored key in the returned metrics: ['epoch', 'step']. HINT: Did you call `log('validation/loss', value)` in the `LightningModule`?
  warning_cache.warn(m)
`Trainer.fit` stopped: `max_steps=2` reached.
[34m[1mwandb[39m[22m: [33mWARNING[39m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")